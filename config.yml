# model initial argument
MODEL:
    embedding_size: 128
    hidden_size: 256
    num_heads: 8
    num_encoder_layers: 2
    num_decoder_layers: 2
    pe: 512
    rate: 0.1

# training args
TRAINARGS:
    use_gpu: true

    # hyper parameters
    train_batch_size: 4
    eval_batch_size: 4
    epochs: 10
    eval_epoch: 2

    # checkpoint
    checkpoint_dir: ckpt
    save_epoch: 2
    save_total_limit: 10

    # tensorboard
    logging_dir: log
    logging_steps: 5
    logging_print: false

    # optimizer parameters
    learning_rate: 0.0001
    warmup_steps: 0
    adam_beta1: 0.9
    adam_beta2: 0.98
    adam_epsilon: 0.000000001
    power: 1.0

# datasets feild. Set the parameters for your custom dataset.
DATASETS:
    data_path: test_resource/data_sample.json  # dataset path. dataset must jsonl type.
    # data_path: test_resource/data_sample.csv
    share_dict:  # Resources shared by multiple processes.
        seq_len: 48
    map_args_list:  # Parameters required for the functions to be applied.
        -
            batch_size: 10  # A unit that groups data into batches. Write 1 if processing one by one.
            worker: 2  # Number of processes to use for multiprocessing.
    train_test_split: 0.05  # size of test datasets. If you don't want to proceed with evaluation, enter null.
    shuffle_seed: 42  # Seed to shuffle the data. If you don't want to proceed with shuffle, enter null.
    input_key:  # keys of input dataset. List[str] or str.
        - input_ids
        - decoder_input_ids
    labels_key: labels  # keys of target dataset. List[str] or str.
    dtype: dict  # Literal['dict', 'tuple']. batch data type.

# disable hydra logging.
hydra:
    run:
        dir: . 
    output_subdir: null
    job_logging: null
    hydra_logging: null

# etc feild and user custom feild
ETC:
    output_dir: model
    tokenizer_path: test_resource/test_tokenizer.json